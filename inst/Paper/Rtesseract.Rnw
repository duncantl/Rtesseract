\documentclass[article]{jss}
\usepackage{thumbpdf,lmodern}
\usepackage{float}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{comment}
\def\Cpp{C\texttt{++}}
\def\fun#1{\code{#1()}}


% Things we should add or make certain are there 
%
% + Examples showing setting some of the options and why they matter
%   e.g. language, white list
% + Querying current options/configuration settings.
%
% + Getting the dimension of the image 
%    See  getImage.R 
% + Limiting to a particular  subregion.
%     See ./setRectangle.R and the SetRectangle example.
%
% + Setting an image after processing it in R first. Have to check this works. 
%     SetImage_raw
% 
%     All names mirror the C++ API, so you can find attitional
%     documentation in the Tesseract documentation.


\author{Matthew B. Espe \\ Data Science Initiative \\ University of California, Davis
  \And Duncan Temple Lang \\ Data Science Initiative \\ University of California, Davis}
\Plainauthor{Matthew B. Espe, Duncan Temple Lang}

\title{\pkg{Rtesseract}: a Package for Optical Character Recognition from \proglang{R}}
\Plaintitle{}
\Shorttitle{}

\Abstract{
  %FIX: Make the first sentence stronger and connect to scanned documents.
  
  Across disciplines, we increasingly desire to recover text and numeric data from scanned documents and images.
  The \pkg{Rtesseract} provides flexible and comprehensive functionality for
  this using optical character recognition (OCR) from within \proglang{R} by interfacing to the Open Source \proglang{C++} OCR library Tesseract.
  At its simplest, \pkg{Rtesseract} allows an \proglang{R} user to recover text from an image as lines, individual words or separate characters..
  Importantly, \pkg{Rtesseract} can also obtain the
  location and size of text elements, possible alternatives for characters, and the estimated  ``confidence'' for each.
  These metadata are often essential for interpreting and reconstructing the text, e.g., for multi-column layout, tables, or even section titles.
  Additionally, the \pkg{Rtesseract} package provides access to much of the Tesseract C++ API, allowing full customization of the OCR behavior,
   and exporting the results in various formats (e.g. PDF).
  Furthermore, \pkg{Rtesseract} includes functionality to visualize OCR results within \proglang{R} itself.
  To further support an OCR workflow from entirely within \proglang{R}, the \pkg{Rtesseract} package also provides access to the leptonica C++ library, adding image manipulation functionality for pre-processing images to enhance the recognition.
  Lastly, \pkg{Rtesseract} includes high-level functionality for recovering structural elements from the page, such as horizontal and vertical lines which are often vital
  for reconstructing data from tables.
  With the \pkg{Rtesseract} package, users are able to create
  a fully programmatic workflow to extract text data into \proglang{R} for
  subsequent analysis.
}

\Keywords{Optical Character Recognition, OCR, R}
\Plainkeywords{}
\Address{
  Matthew Espe, Duncan Temple Lang\\
  Data Science Initiative\\
  University of California - Davis\\
  360 Shields Library\\
  Davis, CA 95616 USA\\
  E-mail: \email{mespe@ucdavis.edu}
}

\begin{document}

\pagenumbering{arabic}

\section{Introduction}\label{intro}
% FIX: we may need to make this section "tighter", i.e. get the essential points
% across in a more compelling manner, and also show the distinction between Jeroen's 
% package and ours, or emphasize that ours was first.
% Everything that is written is good and correct. But it may not be compelling to 
% those who don't already understand the need for metadata.   We may need to spell
% this out with minimally worded examples, e.g. tables with missing cells.


<<setup, echo=FALSE, message=FALSE, warning=FALSE>>=
library(Rtesseract)
library(knitr)

options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)

# Globally set output to be limited to 6 lines
knit_theme$set("print")
opts_chunk$set(out.lines = 6)
opts_chunk$set(highlight = FALSE, background = "white", prompt = TRUE,
               comment = '', tidy = TRUE)


# # the default output hook
# # From https://github.com/yihui/knitr-examples
hook_output = knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
    if (!is.null(n <- options$out.lines)) {
        x = unlist(stringr::str_split(x, "\n"))
        # Truncate long output
        x = sapply(x, function(string) {
            tmp = gsub("[[:space:]] +$", "", string)
            if(nchar(tmp) > 70){
                string = substring(string, first = 1L, last = 66L)
                string = paste(string, '..."')
            }
            string
        })
        if (length(x) > n) {
            # truncate the output
            x = c(head(x, n), "....\n")
        }
        x = paste(x, collapse = "\n")  # paste first n lines together
    }
    hook_output(x, options)
})

# Put this in here to avoid issues with it not being defined later
smithburn = system.file("images", "SMITHBURN_1952_p3.png", package = "Rtesseract")          

@ 

There has been an enormous increase in the amount of text data that we
work with from social media, reports, scholarly articles, job
applications, etc. These often come as plain text or structured
documents such as word-processing files, HTML, or PDF. However, there are
many documents of interest that are only available as scanned or
photographic images (e.g., old books, newspapers, reports).
Optical Character Recognition (OCR) is a technique to infer and extract
characters, words or lines of text from an image, thereby making them
accessible for data analysis.

There are many commercial OCR software tools
currently available (e.g.,
Adobe Acrobat~\citep{acrobat},
PDFPen Pro~\citep{pdfpen},
ABBY~\citep{abbyy},
Google Cloud Vision~\citep{gcv}), 
however most of these return little more than text results.
Although OCR technology has steadily improved over the years, OCR
results are rarely perfect.
When errors in text recognition impact our subsequent analyses,
we need to correct them before proceeding.

Therefore, OCR is commonly part of a workflow that involves
pre- and post-processing steps to verify and increase the accuracy of
the text extraction.
A typical OCR workflow might start with image pre-processing to make the text ``stand out'' prior to OCR.
The image might be cropped, contrast increased, noise decreased, have smudges removed, 
etc. using image editing software.
Then after OCR has been run, we will check if the results make sense.
In some cases there are obvious issues, such as nonsense words or text,
but in other cases we might want to focus our attention on the
areas where the OCR engine had the least certainty.
When we have prior contextual knowledge about the correct matches, such as only
numbers or a fixed set of characters or words occur in the text, we can
leverage this information to correct errors in the post-processing
steps.
When the text is arranged in columns or blocks, recovery of a ``line'' of text will group text together incorrectly.
In these cases, we will want to rearrange the text into the correct order or even recover structural elements such as columns in a table or section titles.
The presence of elements such as lines or other dividers or even the relative size of the text can all be used to make sense of the resulting text.
Diagnostic pieces of information
such as the certainty in the matches, alternative matches, etc., can
be essential in assessing certainty, iteratively improving OCR accuracy or reconstructing
the high-level structure from the individual data values.

When the OCR software does not provide additional information from the OCR process, e.g. location of text or confidence in recognized text,
these steps may be difficult or impossible to conduct.
Using OCR software which only returns text results, our full OCR workflow involves 1)
multiple platforms to perform pre-processing, OCR, post-processing,
and analysis, 2) leveraging pre- and post-processing steps to overcome
limited control over the OCR process itself, 3) limited options to
programmatically control and reproduce each step in the workflow, and
4) limited ability to use those techniques which rely on OCR intermediate metadata, such as confidence measures, alternative matches, location on the page, size of the text, etc.
Hence, if the OCR tool only returns the text results, as is the case with many OCR
tools, OCR's usefulness is limited to cases where the problem is
either simple or well-covered by the default configuration.

In our work extracting and analyzing text data present in images from
\proglang{R}, we required a tool which would overcome the limitations above. 
In our search for solutions, we found Tesseract, a long-standing Open
Source C\texttt{++} library for OCR originally created in 1985 and currently under resurgent
development. We created \pkg{Rtesseract} to access the Tesseract API from \proglang{R} and allows us to tightly integrate pre- and
post-processing steps with the actual OCR and analysis under complete programmatic control within \proglang{R}.
This tight integration simplifies workflows that
otherwise rely on invoking stand-alone applications typically meant
for human interactive use. Furthermore, providing metadata about the resulting
text can aid and inform subsequent analyses. 

The \pkg{Rtesseract} package provides 1) a simple high-level function to
extract the text from one or more images, 2) the ability to also
obtain important auxiliary metadata (e.g., the confidence in each
prediction, the location within the image), 3) customization of
how the OCR is done,
4) extended ability to adjust the image are being recognized, e.g. rotation, focus on subregions, etc.,
and 5) the ability to identify structural elements in the image such as lines or blocks. 
\pkg{Rtesseract} additionally allows \proglang{R} users access to leptonica\citep{leptonica}, the image library Tesseract uses for image pre-processing.
This functionality allows users to provide their own processed image to Tesseract.
\pkg{Rtesseract} also provides
higher-level functionality to visualize the results from the OCR on the original
image.
Lastly, \pkg{Rtesseract} has been
designed to support both the v3.05 and v4.0 versions of Tesseract, with mechanisms to
configure \pkg{Rtesseract} according to the installed versions of
Tesseract, leptonica, and image libraries.


%FIX:  We have to be careful here. It is not the steps in the OCR itself, but the pre and
%post-processing around tesseract, and via run-time variables controlling tesseract.
% The sentence above could be misinterpreted.
%FIX: Smaller point: This set of functionality in the API is quite vague. We may want to provide concrete 
% illustrations of the high-level functionality, e.g., image manipulation (rotation, thresholding)
% customization, post-processing based on the results. It comes next in the paragraph, so adding a
% "including ... below".


\section{Optical Character Recognition using Rtesseract}\label{desc}

% This talks about text, but then doesn't mention GetText (added now)
% and talks about interfaces (plural) rather than functions. 
% So is it the collection of functionality we want to discuss here or just GetText()
% as the paragraph seems to start with the simple case which is GetText().

For many simple problems, merely extracting text from an image in a
reliable manner is sufficient.  \pkg{Rtesseract} provides a simple
function \code{GetText} which allows \proglang{R} users to provide the path to an image file in a
supported format, e.g. TIFF, JPEG or PNG, and to perform OCR on the image
using the default settings (similar to both the \pkg{ocR} and
\pkg{tesseract} packages).

% FIX: The next 2 setences don't quite connect to this paragraph.
% Perhaps lead with these and then introduce the basic text extraction - GetText() which comes
% next anyway. Or else provide a brief overview of the different things we will discuss in this 
% section, starting with simple text extraction.
% It might also be good (and may already be here) to show what can go wrong with, e.g., 2 column
% text with titles centered in the columns and being put/recovered in a line in the wrong column.

For example, suppose we have an image from
an older scientific publication from which we want to extract
information, e.g., Fig.~\ref{fig:base}. We will use this image to
highlight the functionality and common use-cases of the
\pkg{Rtesseract} package. % AWK


\begin{figure}[H]
\includegraphics[width=\textwidth]{\Sexpr{smithburn}}
% Note I added more text to describe the elements of the image to the reader, hopefully
% getting them to think about how we might need them and also setting us up for using them
% later in the text.
\caption{An example image of a scientific paper which contains text we
  would like to extract and analyze. The highlighted text in the second paragraph
  is part of the image. There is also rotated text running vertically down the right of the page.
  There is a table of data with columns and rows separated by lines, and columns left- and center-aligned.
  Some words are in italics and there are also footnotes.
}
\label{fig:base}
\end{figure}

\subsection{Basic Text Extraction}

The most fundamental task for OCR is to convert an image into text.
For this simple task, we use the high-level function \code{GetText}, e.g.,

<<>>=
smithburn = system.file("images", "SMITHBURN_1952_p3.png",
                        package = "Rtesseract")
GetText(smithburn)
@

We pass \code{GetText} the path to a file containing the image.  
%QUESTION: Can we specify a file with multiple pages? And can we specify which page?
By default, it returns the words it finds in the image in the order they %FIX: order needs qualification or some definition
appear.  We sometimes want the results returned as individual
characters (symbols), such as when we are checking how well Tesseract
recognized individual characters, e.g. special or unique characters.
% motivate this better.  e.g. when we will do the word assimilation ourselves.
% or when we want to do something with the words by getting the individual characters
% in a second OCR pass and then comparing.
We do this by providing a value for the \code{level} parameter of the
\code{GetText} function, e.g.,
<<>>=
GetText(smithburn, level = "symbol")
@ 

In other situations, we may full lines (\code{textline}s) of text rather than
individual words. Again, we specify this via the \code{level} parameter,
e.g.,
<<>>=
GetText(smithburn, level = "textline")
@

% I find this confusing. "organizational level"... And we just went through the
% 3 examples. So let's just say we can also use block and para
Modifying the organizational level of the returned text is easy to
achieve with the \code{level} argument in \code{GetText()}.
Possible values include ``block'', ``para'' (paragraph), ``textline'', ``word'' and ``symbol''.


%FIX: Seque that motivates these.
\pkg{Rtesseract} includes three other high-level functions --
\code{GetBoxes()}, \code{GetConfidences()}, and \code{GetAlternatives()} --
to extend the information accessible by the user. Similar to
\code{GetText()}, these functions can take the path of the image file
and the optional \code{level} argument (except \code{GetAlternatives()}
which only operates at the ``symbol'' level, i.e., individual characters).

%FIX: Mentioning these doesn't seem to say why we would want to use them.
% And then we go into API objects. But we are probably just better off
% always using GetBoxes() and accessing the text column from that
% rather than GetText().  And then we don't need the persistent API object.
% It is not typical for novice use to call GetBoxes(), GetAlternatives(), etc.
% but rather just one call and never need the persistent API. The persistent
% API is useful, but more for advanced use, e.g., batch processing, simplifying
% the customization code by doing it just once (but even this is not a big deal).
% So the persistent API might be better coming later and we focus on 
% GetBoxes(), GetAlternatives(), etc. immediately/next, saying what they return and 
% why you want to use them.
% It is useful to use the API object when we want to plot the results as we will avoid
% multiple Recognize() calls (is this true???). This is an optimization issue, but important. But may not
% cause us to introduce the API object interface so early. It seems more complex than GetText()
% or GetBoxes()

\subsection{Creating a persistent API object}

This practice of calling \code{GetText()} followed by another, e.g.,
\code{GetBoxes()} for the same file is a common occurrence when working
interactively with \pkg{Rtesseract}. Yet, it has a distinct
inefficiency - we repeat the OCR recognition for the same file merely
to export a different aspect of the output from the OCR engine. Since
OCR is reasonably computationally intensive, we would like to perform
the OCR once and then query each aspect we want (i.e., the text, the
confidences, the alternatives, \ldots) from this single OCR operation.

The \code{tesseract()} function creates an instance of the OCR engine
(an object of class \code{TesseractBaseAPI} in \proglang{R}).  We can repeatedly
query this for the text, boxes, confidences and alternatives without
repeating the OCR.  This is the preferred method when multiple pieces
of information are needed from the same image.
% Say we can change its settings (i.e. variables), and also have it focus on 
% subregions of the image, and even reuse the instance to process multiple
% images having customized the settings.

We typically call \code{tesseract()} by specifying the path to the file
and any other options controlling how the OCR will be performed
(explained in more detail below in ``Advanced Use''):

<<>>=
api = tesseract(smithburn)
@
% We didn't show any other options, yet we said we could.

We then pass the object returned by \code{tesseract()} to each of the
four high level functions rather than the file name, e.g.,
\code{GetText(api)}, \code{GetBoxes(api)}, \code{GetConfidences(api)}.
By default, the call to \code{tesseract()} doesn't perform the OCR
immediately.  % Since R has lazy evaluation, we could refer to this as "lazily" to exploit the readers'
% familiarity with R's lazy eval ?
Instead, it is only done when we request any results,
e.g., calling any of the four functions above, or explicitly calling
the \code{Recognize()} function with the \code{TesseractBaseAPI} object
as an argument.

% This next sentence seems like we are using it to illustrate a point, not that 
% its use is the "best" workflow.
We can use this \code{TesseractBaseAPI} object to illustrate how
\pkg{Rtesseract}'s ability to retrieve more than just the recognized
text from an image can extend an OCR workflow. To continue the example
above, after extracting the text from the scientific publication, we
might want to check the results for inaccuracies. However, checking
word by word manually is both tedious and error-prone. Rather, it
seems reasonable to focus our attention initially on those words with
the lowest confidences (note: ``confidence'' as used by Tesseract is a distance based measure of total similarity to the predicted character):
% Following the lengthy comment above, this seems somewhat contrived. We can just call GetBoxes(smithburn)
% and we have everything we need. This is what Jeroen's package does as it is sufficient for almost all
% typical cases.

<<>>=
confs = GetConfidences(api)
round(sort(confs)[1:10], 1)
@ 
% Do we need to explain what the output is? The "recognized" words and their confidences.

Clearly, some of these do not look correct. To further investigate, we
might want to see where these low-confidence words occur on the page
by extracting the bounding boxes,

<<>>=
bbox = GetBoxes(api)
@

\code{GetBoxes()} extends the information returned by
% So why don't we just call GetBoxes() directly once and extract the subset of information we want?
% We seem to be recommending not best practices and making things more complicated.
\code{GetConfidences()} by including the left, top, right, and bottom of
the bounding box around the word, symbol, textline, etc. Using these,
we can sort by the confidence,

<<>>=
bbox[order(bbox[,"confidence"])[1:10],]
@

At first glance, it appears the low confidence words might be grouped
together in similar areas on the page, with the far right side of the
page (``left'' and ``right'' values of approximately 3800-4000)
showing up often in the lowest confidence results.

Since we have access to the confidences and bounding boxes in \proglang{R}, we
can explore this conjecture by treating these objects just like any other
data. To explore our hypothesis that these terms are grouped left to
right, we can group the words into bins by their location left to
right using the \code{cut()} function,

<<>>=
lrGroups = cut(bbox[,"left"],
               breaks = seq(min(bbox[,"left"]),
                            max(bbox[,"left"]),
                            length.out = 11))
               
@ 

And then calculate the median confidence inside each group,

<<out.lines=8>>=
tapply(bbox[,"confidence"], lrGroups, function(x) round(median(x),1))
@

Most intervals have a median confidence of approximately 91, except the last one
which is 57.
This verifies our suspicion that there are issues with the right
side of the page. Looking at the image (Fig. \ref{fig:base}), we can
see there is rotated text in the margin that identifies when and where
the PDF was downloaded. This text is not being recognized well by the
OCR (as it only works with a single text orientation), but is also not part of the journal article in which we are
interested. Because this text crosses several textlines,
post-processing without location information would involve searching
for ``junk'', e.g. non-valid words such as ``Isosziim'', at the end of
each text line. However, we can easily exclude this text in
post-processing with location information in hand. Additionally, while
we could crop this image using editing software outside of \proglang{R}, we will
show later how this region can be excluded from the OCR recognition step
using \pkg{Rtesseract} without leaving \proglang{R}.

\begin{comment}
%% Not a great example
To continue, suppose we also wanted to extract the author's
name. Using some prior knowledge that this information is present at
the top of each page, we can focus on the first text line,

<<>>=
GetText(api, level = "textline")[1]
@ 

We know that the lower case letter in abbreviated first and middle
names is a mistake since the letter must be upper case.
But we might not be sure what the correct letter should be.
Should it be an ``O'', or a ``C''?
Helpfully, we can inspect the alternative matches, in this case for
the sixth character in the OCR output,

<<>>=
GetAlternatives(api)[6]
@ 

which reveals that ``C'' was a close second in this case. The
alternatives which are capital letters are ``O'' and ``G'', and both have
substantially lower confidences than ``C''. Therefore, we can be reasonably
certain that the author's name should be ``K. C. Smithburn'' without
inspecting the original image.

This example illustrates both the workflow of using OCR to extract
text and highlights why having access to information beyond just the
text is so helpful to a full OCR workflow, if not crucial for increased
accuracy and error checking.
\end{comment}

\subsection{Visualizing OCR results}
% What is this? Different paragraph and section, so need explicit connectivity.
In this OCR workflow, we find that visualizing the confidences is
helpful for quickly inspecting \pkg{Rtesseract} results for
issues. Therefore, \pkg{Rtesseract} includes a plotting method for OCR
output. By default, calling \code{plot()} on a \code{TesseractBaseAPI}
object will create a plot which includes the original image overlaid with
the bounding boxes of the recognized elements (words, letters, lines, \dots),
colored (and optionally filled) by relative confidence. See Fig. \ref{fig:rtess_plot} for an example. Plotting the
results is especially helpful when adjusting the OCR configuration
settings to improve accuracy, which we discuss below.

\begin{figure}[H]
<<echo = FALSE>>=
plot(api, main = "")
@
\caption{\pkg{Rtesseract}'s plotting function allows the user to
  overlay the scanned image with the bounding boxes surrounding the
  recognized elements, optionally colored according to the boxes'
  associated confidences.}
\label{fig:rtess_plot}
\end{figure}
%FIX: The contrast between the different colors of the boxes isn't great. Perhaps we need to use
% a red to green scale.  Also, do we need to put a legend on the plot? Probably!

\subsection{Zooming and subsetting an image}
%FIX: This probably needs more motivation. It is true, but does not necessarily explain why.
% When and why is it useful. Just the poorly recognized regions? And what if they are not contiguous?
% But what are other reasons?
It is sometimes useful to ``zoom in'' on a part of the image to inspect a region that is not being accurately recognized.
%FIX: This is potentially confusing the image and the plot.  We are not zooming in on the image for
%OCR, but the results of the OCR.
For visualizing a sub-area of the original image, we use the \code{plotSubImage()} function.
For example, we can find a low confidence word,

<<>>=
bbox = GetBoxes(api)
i = which(bbox$confidence < 25)[1]
bbox[i,]
@
% Why are we only looking at the first of these? And what is it? How did we decide that?
% Say that plotSubImage() is for individual words, apparently.
% Do we really want to talk about it at all?
% What about showPoints() (in ReadPDF?) to highlight the points on an existing plot.
% Or your plotSubsets()

and then we can zoom in on that word,

\begin{figure}[H]
<<>>=
plotSubImage(bbox[i, ], img = png::readPNG(smithburn))
@ 
\caption{An example of plotting a zoomed-in segment of an image using \pkg{Rtesseract}}
\label{fig:zoom}
\end{figure}
% FIX: zoomed-in is a colloquial term.
% What does this show us? It doesn't help me much by itself?  We need context?
%FIX in code: We should have a method for the data.frame so we don't need as.matrix() and the [, 1:4].

There are times when there is only a part of the image that we want to recognize with OCR.
In this case, we may want to avoid the computations involved in recognizing the entire contents of an image when we are only interested a piece of it.
% FIX: Perhaps add that the OCR will be confused by other aspects of the larger image and so we need
% to restrict its view to the subregion. Give a motivating example.
We could crop the original image, create a new file containing only the area of interest, and then
call \code{tesseract()}  % FIX: or GetBoxes()
on this image.
However, it would be better to focus only on the area of interest without modifying the original image.
%FIX: Why???
For this, \pkg{Rtesseract} includes the \code{SetRectangle()} function. For example, if we know we are only interested in the table on the lower half of the page, we can recognize only this area. First, we can locate where the top of the table is by finding the word ``TABLE'',

<<>>=
bbox[grep("TABLE", bbox$text),]
@ 

Then we reduce the recognition region to only the rectangle below this word using the
\code{SetRectangle()} function,
with the dimensions specified as left edge, top edge, width of box, and height of box 

<<>>=
SetRectangle(api, dims = c(0, 1864, 4000, 6000 - 1864))
GetBoxes(api)
@
% FIX:
% 1. Need to explain 0,0 is top-left corner. This needs to come in easly and say how plot() is taking
% care of all of this for us. We could mention this when explaining the output of GetBoxes()
% 
% 2. Where did the 400 and 6000 come from. We should tell people how to calculate them?
%  dim(api)
%    or  dim(GetImage(api)) or GetImageDims(api) 
% 3. And can we do better and focus on the table and exclude the bits within the lines and exclude
% the text. We can do this semi-manually, but also note that we can find the lines.
% If we just say we know where the lines are (which we can compute), then we can punt on the
% image dimensions for now.

Although this example only highlights basic uses of the ability to restrict the recognized area, we will show later how we can use \code{SetRectangle()} in conjunction with image manipulation to handle more complex tasks.
Setting the recognition area can be helpful in cases where there are
multiple languages in the same image, or when one area is most
accurately recognized with settings suboptimal for other areas. 

\section{Controlling the OCR engine behavior}

Thus far, we have focused on functions in \pkg{Rtesseract} which allow
accessing and manipulating the results of running the OCR engine on an
image. In many non-trivial cases, we might want to control/customize the % was manipulate.  Chose best term here.
behavior of the OCR engine itself rather than only the output. For
these purposes, the \pkg{Rtesseract} package exposes full access to
the methods in the \code{TesseractBaseAPI} object performing the OCR.
This allows fine-tuning of the OCR engine for specific cases.

\subsection{Page Segmentation Mode and OCR engine}

There are two ``meta-variables'' that control how Tesseract recognizes the arrangement of text and which OCR engine is used,
\code{pageSegMode} and \code{engineMode}. Since adjusting these two
variables can have a large impact on the results of the OCR, they deserve special mention.

The first -- the Page Segmentation Mode --
adjusts how Tesseract recognizes the layout of the text in the image.
Correctly identifying the text layout is an important first step to accurate OCR.
As we have seen previously, text cannot be properly recognized when it is arranged vertically where Tesseract expects horizontal layout.
Adjusting the Page Segmentation Mode variable allows us to directly tell Tesseract how to
expect the text to be arranged, e.g. as a single block of text, sparse
text, arranged top to bottom (e.g., the vertical arrangement of many
Asian texts) or even in a circle (e.g., the text on the rim of a
coin), etc.
Included in the available options are to have Tesseract
attempt to automatically detect the text orientation, the script used, % What do we mean by "script"?
or both. %more here
The \pkg{Rtesseract} package uses the
defaults in the Tesseract \Cpp API and assumes the input is a single
block of text (\code{PSM\_SINGLE\_BLOCK}), though in our
experience OCR can be significantly improved in some cases by changing
the Page Segmentation Mode. We have found for most problems
\code{PSM\_AUTO}, which attempts to detect the correct page
segmentation automatically, is a good default, though any of the
currently \Sexpr{length(grep("^PSM_", ls('package:Rtesseract')))}
available Page Segmentation modes might be best for different
applications.
%% Curiously,
%% \code{PSM\_AUTO} is the default for the tesseract command line tool
%% but not for the API.
The current Page Segmentation Mode can be found by calling
\code{GetPageSegMode()} on a \code{TesseractBaseAPI} object,

% We should check/have a circular text in the package (not in the paper.)
<<>>=
GetPageSegMode(api)
@

Page Segmentation Mode can be set in \pkg{Rtesseract} in multiple
ways: in the call to \code{tesseract()},

<<eval = FALSE>>=
api = tesseract(smithburn, pageSegMode = "PSM_AUTO")
@

in the \code{...}  arguments for any of the high-level functions,  %% @MATT - do we define high-level clearly early on?
e.g.,

<<eval = FALSE>>=
GetText(smithburn, pageSegMode = PSM_AUTO)
@

or by using \code{SetPageSegMode()} on an existing
\code{TesseractBaseAPI} object,

<<>>=
GetPageSegMode(api)
SetPageSegMode(api, mode = PSM_AUTO)
GetPageSegMode(api)
@

The available Page Segmentation Modes are listed as enumerated values
in the \pkg{Rtesseract} package, and can be found with,

<<>>=
Rtesseract:::PageSegMode

One can specify a value for the \code{pageSegMode} parameter
using 1) the \R{} variable names (e.g., \code{PSM_CIRCLE_WORD}),
2) the symbolic name in quotes and in upper or lower case and with partial matching (e.g., "circle"),
or 3) the corresponding number, which is both less readable
and less reliable as the values may change across versions of 
tesseract.


@ 

The second of these ``meta-variables'' is the Tesseract Engine Mode, which
determines which OCR engine is used to recognize text.
Tesseract operates by recognizing elements of individual characters, and different OCR engines recognize these elements using different algorithms.
Tesseract ships with additional OCR algorithms which can be used individually or in
combination with Tesseract's original line-finding algorithm.
For Tesseract versions 3.05+,  %@MATT Does this mean versions >= 3.05 or. We should use this
                               %notation or better, use explicit english. And is this <= 4.00
an additional OCR engine ``Cube'' is included, and for Tesseract versions 4.00+ a Long Short-term Memory (LSTM) neural network-based OCR engine is included.
Although the use of the other OCR engines in combination with Tesseract can potentially
increase accuracy, their use can add significant computational load
(between 6--10x)  %@MATT Evidence for this.
and require appropriately trained language files, % What does this mean? We haven't introduced it yet?
so they are not used by default by \pkg{Rtesseract}'s OCR functions. For
a comparison of the accuracy and performance of each of these options,
please refer to
\url{https://github.com/tesseract-ocr/tesseract/wiki/4.0-Accuracy-and-Performance}.
%@MATT: Perhaps add this to the bibliography.

The current Engine mode can be found by calling \code{oem()} on a
\code{TesseractBaseAPI} object,

<<>>=
oem(api)
@ 

Similar to Page Segmentation Mode, the Engine Mode can be set in
several ways. % This is unclear - do you mean by R variable, "string" or numeric value, but not via
              % different functions.
However, unlike Page Segmentation Mode, the Engine Mode
needs to be set at initialization, which means you cannot set it on an
already initialized \code{TesseractBaseAPI} object. 
%@Duncan, this correct?
%@MATT: Actually, apparently (surprise to me) we can with, e.g.
%    api = tesseract()
%    oem(api)
%    Init(api, engineMode = OEM_TESSERACT_LSTM_COMBINED)
%    oem(api)
% What this does to the state of the tesseract API object, I don't know.
% Does it clear the image? Yes.
% It doesn't reset all variables.
%
% Now added an oem<-() method.


%FIX:  I think we need to explain more about what the segmentation and engine mode actually
% control so people know why they would like to explore them.


The Engine Mode can be set either in the call to \code{tesseract()}

<<eval = FALSE>>=
api = tesseract(smithburn, engineMode = OEM_TESSERACT_ONLY) 
@

or in the call to a base level function when calling with an image filename,

<<eval = FALSE>>=
GetText(smithburn, engineMode = OEM_TESSERACT_ONLY)
@ 

Notice that \code{GetText()} is being called with the file
name. Attempting to set the engine mode on an already created
\code{TesseractBaseAPI} object will have no effect, e.g.,
\code{GetText(api, engineMode = "OEM\_CUBE")}.
%FIX: We could fix this if we want! 


\subsection{Image Manipulation and Pre-processing}

\subsubsection{Rotating Text}

<<child = "rotateZoom.Rnw">>=

@ 

\subsubsection{Pre-processing the Image}

Consider again the image in Fig. \ref{fig:base}.
When we perform OCR on this using only Tesseract, Tesseract treats the highlighted text in "yellow" as a dark rectangular block and ignores it. The LSTM engine in Tesseract versions 4.00+ can handle situations like this better,
but at the time of writing this feature is still actively under development and is much less stable than the original algorithm.

For example, some of the words in the image present in the highlighted text are ``Drs. Dick and Haddow.''
Are these in the words returned by Tesseract?

<<>>=
t0 = tesseract(smithburn, engineMode = "tesseract_only")
w0 = GetText(t0, level = "textline")
grep("Drs. Dick and Haddow", w0, value = TRUE)
@

%@MATT  have we explained the engineMode value tesseract_only? and why we would use this.
% Also, we can use GetThresholdedImage() after Recognize() to look at the contents of the image 
% from which Tesseract will extract the words.  This clearly shows the large band/rectangle over these
% words and why they are not extracted. Then we can play with the thresholding level.


%% Tesseract 4.00-beta does get this!!
It seems not. In this case, the text under the highlighted area is
being lost. We need to pre-process the image before we pass it to Tesseract,
removing the highlight but not the text, in order to  recover this text.  There are many options for
pre-processing images, including Imagemagick, OpenCV, Leptonica, or
even \proglang{R} itself operating on the image as a matrix or array. Leptonica
is already used by Tesseract internally to do some pre-processing of
the image, e.g., de-skew, remove lines. Tesseract conducts OCR
using a \code{Pix} object (Leptonica's native image object format) created by Leptonica during this
pre-processing. It is  computationally efficient to
pre-process the image as a \code{Pix} object in memory to be handed
directly to Tesseract rather than write the image to an intermediate
file on disk, only to be read and converted again to a \code{Pix}
object. For this reason, we have included bindings to the Leptonica
library in the \pkg{Rtesseract} package for basic image processing.

To remove the highlight using Leptonica from within \proglang{R}, we
need to convert the image from color to gray-scale or 8 bits per
pixel, and thereby convert the yellow area to a light gray.  We do
this with the \code{pixConvertTo8()} function.

<<>>=
pix = pixRead(smithburn)
pix1 = pixConvertTo8(pix)
@

We can test if this is sufficient to recover the text under the yellow
highlighted region by calling \code{tesseract()} directly with the
modified image:

<<>>=
t1 = tesseract(pix1)
@
%@MATT: Note the engineMode is different than above.

Then we call, e.g., \code{GetText()} as before, e.g.,

<<>>=
w1 = GetText(t1, level = "textline")
@

and check whether the words we see under the highlighted text are in
the words returned by tesseract for this modified image:

<<>>=
grep("Drs. Dick and Haddow", w1, value = TRUE)
@ 

Indeed, they are there.

\subsection{Using Leptonica to identify lines}

A primary motivation underlying the \pkg{Rtesseract} package is that we need context from the rendered page to make sense of the results of the OCR.
Rather than just recovering the text from the image, generally, we want information
about the location and size for each character, word or line
identified by Tesseract so that we can use it to intelligently
reconstruct the high-level content, e.g, columns of text, tables with rows and columns. 
This also includes identifying document and section titles in the document based on the size of the text.  
% FIX This is doesn't follow - a bait and switch. :-)
In this example, we'll look at the locations of lines on the page in order to be able to interpret data within tables.  Again, consider the previous image in Fig. \ref{fig:base}.

We can clearly see in the image that the data in TABLE 1 are arranged by row and column.
The column headers are separated by two horizontal lines spanning the
width of the text on the page.  The final 4 columns have two column
headers that span all 4 columns.  The header in column 4
(``NO. STRAINS ISOLATED'') is on 3 separate lines but clearly
differentiated from the other data rows in the table due to the
horizontal line separating the header from the data rows.

Firstly, the \code{tesseract()} function allows us to get the
horizontal and vertical locations of the words on the page.  So we
could use these to attempt to recover the tabular nature of this data.
In this particular example, we can do this effectively and relatively
simply.  However, in images with tables that have center-aligned
columns (e.g. the ``Year Isolated'' column) or with many empty cells, this is more
complicated and can lead to ambiguity regarding which column a cell
belongs.  Instead, we can separate the values into their respective
columns much more effectively if we can identify the vertical lines
between columns.  Similarly, we can separate the column header cells
from the data cells below if we know the locations of the horizontal
line(s) that divide them.  Furthermore, we can determine the end of
the table and the continuation of the regular text below it if we can
find the final horizontal line in this example.

Given the motivation of finding the vertical and horizontal lines on
the page, let's explore how we can do this with \pkg{Rtesseract}.
Unfortunately, we have to do this outside or separately from the OCR
stage of recognizing the text.  The OCR process Tesseract performs
actually identifies and removes the lines from the image before it
recognizes the remaining text on the image.  This aids the accuracy of
the OCR.  However, it does not allow us to query the locations of the
lines it removed.  Accordingly, we must replicate approximately the
same computations it performs to identify the lines and their
locations.  Fortunately, the basic ideas of this process are described at
\url{http://www.leptonica.com/line-removal.html}.  The \pkg{Rtesseract}
package provides corresponding (and higher-level convenience)
functions to effect the same computations.

Our aim here is to simply identify the locations of the horizontal and
vertical lines in the image before or after we perform the OCR.
Note that these steps are entirely independent of the OCR.
When we pass the original/raw image to OCR without modifying
it,
Tesseract will do many of the same computations we do, but for the purposes of removing the lines to increase OCR accuracy. %% Does not make sense

\subsubsection{Horizontal Lines}

We start by reading the image:

<<>>=
p1 = pixRead(filename = smithburn)
@

%NOTE: getLines() and findLines() now take the name of the file containing the image
% and does the conversion to 8-bit and then binary thresholding. 
% So we could drop some of this. However, it is also good to show that this is possible
% when the defaults don't work or we need more control over the preprocessing.
%
% We should  probably talk about thresholding and color manipulation earlier and as a separate
% example, e.g. the wine images where the titles of some of the sections are in a light color and
% can be omitted if we leave it to tesseract to do the thresholding for us. We need a lower
% thresholding.


We immediately convert the image to gray-scale with \code{pixConvertTo8()}:

<<>>=
p1 = pixConvertTo8(pix = p1)
@

(Note that this creates a new \code{Pix} and allows the previous one to be
garbage collected as we assign the new image to the same \proglang{R} variable.)

The next step is to correct any skew or orientation in the image.
This can be important in detecting horizontal and vertical lines that
may be at an angle because the scan was done with a slight rotation.
There is a function \code{deskew()} to do this.  However, we will show
the steps as they illustrate some of the Leptonica functions available
in the \pkg{Rtesseract} package.

To detect the skew, we must first create a separate binary image using
\code{pixThresholdToBinary()}.

<<>>=
bin = pixThresholdToBinary(pix = p1, threshold = 150)
@ 

Then we call \code{pixFindSkew()} to
determine the actual angle of the skew.

<<>>=
angle = pixFindSkew(pix = bin)
@ 

Then we rotate the original image by that skew angle:

<<>>=
p2 = pixRotateAMGray(pix = p1, angle = angle[1]*pi/180,
                     grayVal = 255)
@

When the page is deskewed, areas in the corners of the page have to be filled to maintain a rectangular image.
The 255 in the call to \code{pixRotateAMGray()} corresponds to white and
means that any pixels that are ``revealed'' by rotating the ``page'' are
colored white.
We can specify any
gray-scale color value we want here between 0 (black) and 255 (white).

With the reoriented image, we call the \pkg{Rtesseract} function
\code{getLines()} to get the coordinates of the lines in the image.
We'll first extract the horizontal lines.  For this, we pass the image
and then describe a ``mask'' or box that helps to identify contiguous
regions in the image corresponding to the lines we want.
This mask is important for identifying lines that might be slightly broken due to poor quality scans of light or thin lines on the page.
Broken lines with white space between each other less than the width (horizontal) or height (vertical) of the rectangle are considered to be connected. 
Hence when the rectangle is wide (horizontal) or tall (vertical), lines are combined more agressively.

For horizontal lines, we want wide and vertically short boxes.
We describe this box with the width (horizontal) and height (vertical) dimensions of the box, e.g.,

<<>>=
hlines = getLines(pix = p2, hor = 51, vert = 3)
hlines
@
This returns a list with each element being a matrix identifying the (x0, y0) and (x1, y1) coordinates describing 
the segments that are considered as a single line.
This allows small gaps between the segments. These gaps may be structural or due to errors in the
scanning and image processing.

We can add these lines to the display of the image to verify they
appear where we expect (Fig. \ref{fig:lines_id}).

%FIX: See the lines() method now available/added. See lineSegments.R
\begin{figure}[H]
<<fig = TRUE, results = "hide">>=
plot(p2)
lapply(hlines,
       function(tmp) {
           lines(tmp[, c(1, 3)], nrow(p2) - tmp[, c(2, 4)], col = "red",
                 lty = 3, lwd = 2)
       })
@
\caption{The result of identifying lines using the \code{getLines} function colored in red over the original image.}
\label{fig:lines_id}
\end{figure}
%@MATT: I changed the code here to treat tmp as a matrix, i.e., add the , to indicate all rows.
% In this case, the lines are all single segments, but in general they may be multi-row matrices
% I also added a class and a method for showing the lines so that people don't have to do this computation
% directly. So we may want to remove it from the text, or show it and say that it is done via the method
% showLines()

Note that we had to subtract the y coordinates from the number of rows
of the image as \proglang{R} plots from bottom up and the coordinates for the
lines are relative to row 1 and go down.
%FIX: We should make this observation much earlier in the text and discuss the tesseract image model in
% relation to the results in GetBoxes().

For those intersted, we include a vignette with \pkg{Rtesseract} which illustrates how the \code{GetLines()} function leverages several lower-level functions in \pkg{Rtesseract} to identify lines on the image.


\begin{comment}

\code{getLines()} performs several operations on the image.
To illustrate how the lower-level functions in \pkg{Rtesseract} to can be leveraged to address special cases, we will describe how \code{getLines()} works.
First, we use the \code{findLines()} function to return an image that contains
either the horizontal or vertical lines,

<<<>>=
hLines = findLines(pix = p2, hor = 51, vert = 3)
@

We specify the \code{Pix} to process and then the horizontal width and
vertical height (in pixels) that define the ``mask'' window.  As explained previously, this
``mask'' is used to fill in gaps in potential lines within the region.
We use Leptonica to fill in gaps within the masked area if the rest of the pixels within the region are not white.
Internally, \code{findLines()} uses the function \code{pixCloseGray()} to do this.
%% Might not need this info

We can look at the resulting image to see what lines it detected:

\begin{figure}[H]
<<>>=
plot(hLines, invert = TRUE)
@
\caption{An example of detecting lines on a page using the \code{getLines()} function in \pkg{Rtesseract}.}
  \label{}
\end{figure}

Due to differences between \proglang{R} and Leptonica, the \code{Pix} object's background has values of 0, which corresponds to black in \proglang{R}'s plotting functions.
Adding the option \code{invert = TRUE} in the call to \code{plot} will invert the \code{Pix} so that the image in \proglang{R} is as we would expect,
with the background white and the lines black.
We see the 4 primary lines that span most of the width of the image.
The second one down (at y = 3600) has several short gaps.   We
also see many short line segments above the first line and to the
right of the image.   The ones on the right correspond to parts of the
letters in the rotated text in the image indicating ``Downloaded from
http://www. ...''.  If we made the vertical height of our mask larger,
we may discard these but at the risk of including other elements.

We are looking for rows in the image that have many black pixels (with
value 1).  Currently, \code{findLines()} returns an image (Pix
object).  By default, this shows the potential lines and removes
everything else.  (We can also return the image with the lines removed
and the text remaining.)  We convert this to a matrix of 0s and 1s in
\proglang{R} using \code{pixGetPixels()}:

<<>>=
pMat = pixGetPixels(hLines)
@

We are looking for lines which span more than a certain threshold.  
This threshold will be context specific, but we can find a
suitable one via some experimentation.
An initial guess of 1000 pixels resulted in discarding two shorter segments which we are interested in.
So we will relax the threshold of 1000 black pixels.
As a second guess, we'll use 10\% cells in a column:

\begin{figure}[H]
<<>>=
longLines = rowSums(pMat) > 0.1 * ncol(pMat)
plot(hLines, invert = TRUE)
abline(h = nrow(pMat) - which(longLines), col = "red")
@
\caption{An example of locating lines on a page. After locating any possible lines, we have colored red any row with greater than 10\% of the cells containing a black pixel. This restricts our attention to the longer lines.}
\label{fig:highlight_lines}
\end{figure}

As you can see in Fig. \ref{fig:highlight_lines}, we now detect these two shorter lines and have avoided identifying other additional lines
(except the original 4 longer lines).
However, now we have 74 rows in the matrix that satisfy this criterion.
Yet we see only 6 lines on the plot.

Let's look at the row numbers for these 74 rows:

<<>>=
which(longLines)
@

It may not be clear, but these are grouped together.  Each of the red
lines we see on the plot is actually a collection of multiple adjacent lines.
In other words, the line's thickness on the page results in the line spanning multiple rows of the matrix.

%\begin{comment}
This is clearer if we compute the difference between the row numbers:

<<>>=
diff(which(longLines))
@

We see a collection of 1s meaning that these are adjacent rows, then a
large jump between row numbers, followed by a collection of 1s and on.
So it is clear these are grouped.
%% \end{comment}

We want to combine adjacent lines
into a single group and have a separate group for each conceptually separate
line on the image.

%% \begin{comment}
One way to do this is run length encoding (RLE):

<<>>=
rle(diff(which(longLines)))
@

Another way is very similar but more explicit and gives us a little
more control.
%% \end{comment}

We find where the differences between successive row
numbers is greater than 2 and then we run a cumulative sum which gives
us group labels:

<<>>=
lineGroups = cumsum( diff(which(longLines)) > 2 )
@

We can use \code{lineGroups} to split the rows into groups

<<>>=
rowNums = which(longLines)
combinedLines = tapply(seq(along = rowNums), c(0, lineGroups),
            function(i) pMat[rowNums[i], ])
@

Note that we added 0 to the start of the vector \code{lineGroups} since \code{lineGroups} has one
less element than \code{longLines} since we were computing pairwise differences.
The computation is a little awkward.  We split the indices of \code{rowNums}
and then have to use those to index back into \code{rowNums} to find out
which rows of \code{pMat} we need to extract for this group.

Lastly, we put the average of the row numbers of each group as
the name of that element.
This will be convenient for mapping this
back to the general vertical area.

<<>>=
names(combinedLines) = tapply(seq(along = rowNums), c(0, lineGroups),
                   function(i) as.integer(mean(rowNums[i])))

@ 

\code{combindedLines} is a list, with each element a matrix containing rows we suspect belong to the same horizontal line on the page.
For each of these, we want to combine them and identify the coordinates of the horizontal start and end of a line.
This is complicated by the fact that each element may contain a single line or multiple lines segments.
We have only considered the percentage of the row which is filled by black, and will now need to create a rule to distinguish between a solid line and a line segment. 

Let's consider the first element of \code{combinedLines}.  It has 14 rows and 4050
columns.

%% \begin{comment}
  It is useful to query how many 0s and 1s there are?

<<>>=
table(combinedLines[[1]])
@

These are about equal: 27000 0s and 30000 1s.  (In this setup, 1 is
black.)  So there are many white pixels even though the black pixels
appear as solid line on our plot.  But that was because we just drew a
straight line at these vertical positions, regardless how many pixels
were white.  Of course, we selected these rows because they had many
black pixels.
%% \end{comment}

We can start with the assumption that this element contains either a single line or line segments, as opposed to a series of parallel lines. 
We can aggregate across the adjacent rows to form a single aggregate row using \code{colSums()}:

<<>>=
rowNcells = colSums(combinedLines[[1]])
@

If this is a solid line across most of the page, we'd expect most
pixels to be filled, perhaps with a few short gaps.  Each element of
\code{rowNcells} contains the number of cells in each column that had a value of 1 (black in this setup).
This can vary from none of the cells to all of the cells containing a value of 1. 
%% This might be 1 or 14, with only one row or all of them having a black pixel.
%% We might look at these counts or just consider whether any row has a black pixel in this column.
From this information, we are looking for a rule to mark a pixel in the aggregated line as black (solid line) or white (line segments).

If we look at the vector \code{rowNcells}, we see a lot of 0s at the
beginning and the end.  These are the page margins where the line is not
present.

If we want to threshold this aggregate line to have a black pixel in a
column if \textit{any} of the pixels in the adjacent rows defining it
has a black pixel, we can do this with

<<>>=
rle(rowNcells > 0)
@

We get the two margins and a long run of 2895 pixels.  If we require
at least half of the 14 rows to have a black pixel in a column, we get
the same groups:

<<>>=
rle(rowNcells >= 7)
@

%% \begin{comment}
Let's consider one of the shorter lines.  These are elements 2 and 3
of \code{combinedLines}.  Again, we compute the column sums for the adjacent
lines in this group:

<<>>=
rowNcells = colSums(combinedLines[[2]])
table(rowNcells)
@

Most are 0.  There are 9 adjacent rows in this group.  We see no
columns that have at least one black pixel but less than half the
number of rows.
%% \end{comment}

Let's draw points at each of the columns that have a
black pixel in at least half the rows:

\begin{figure}[H]
<<>>=
z = which(rowNcells >= 4.5)
plot(hLines, invert = TRUE)
points(z, nrow(pMat) - rep(as.integer(names(combinedLines)[2]), length(z)),
       col = "green", pch = 16, cex = 0.5)
@
\caption{An example of identifying lines.}
\label{}
\end{figure}

This appears to capture the short horizontal line spanning the four
columns on the right of the table.  So this appears to be a reasonable
approach, at least for this image.
\end{comment}

\subsubsection{Vertical Lines}

We can follow the same process to identify the locations of the
vertical lines.  We've already deskewed the original image, so we don't
have to do that.  We can call \code{findLines()} and this time specify
a window that is tall and narrow to capture the vertical parts.  We
should experiment with these values for the \code{hor} and \code{vert} parameters
defining a vertical line segment.  One thing to note is that
some characters have vertical components more than horizontal components,
e.g., 1, L, M.  If we make the height of the window too small, we'll
identify some of these as vertical lines, i.e.,  false positives.

<<>>=
v = findLines(pix = p2, hor = 3, vert = 101)
@

Again, we can plot this and see where the lines are.
%FIX: Should we do this?

\subsubsection{Removing the Lines}

We don't have to remove the lines before we pass the image to
Tesseract as Tesseract will remove them for us.  However, it is
interesting to see how well we can do.
%FIX: This is unclear. 
Instead of finding the
locations of the lines (with \code{getLines()}), we'll remove them
from the image.  The initial steps use the same basic approach as finding the lines

<<>>=
p1 = pixRead(smithburn)
p2 = pixConvertTo8(p1)
p2 = deskew(p2)
p3 = findLines(p2, 51, 1, FALSE)
p4 = findLines(p2, 3, 101, FALSE)
@

However, rather than extract the line coordinates, we use
\code{pixAddGray()} to overlay the images with the horizontal and then
the vertical lines onto the original image.
This removes the lines from the original image.
% FIX: Perhaps we need more of an explanation here.

<<>>=
p = pixAddGray(p2, p3)
p = pixAddGray(p, p4)
@

We can plot the resulting image (Fig. \ref{fig:remove_lines}) and compare it to the original image (Fig. \ref{fig:base}).

\begin{figure}[H]
<<>>=
plot(p)
@
\caption{An example of removing lines from an image using the \code{pixAddGray()} function in \pkg{Rtesseract}.}
\label{fig:remove_lines}
\end{figure}

The horizontal and vertical lines are essentially gone.  We can
certainly see light grey ``smudges'' in some places where the vertical
lines were.  We also see some short horizontal line segments
remaining.

We can experiment with the horizontal and vertical values we pass to
\code{findLines()}.  Increasing the horizontal width from 51 to 75
cleans up the horizontal lines.

We can also change the thresholds for creating the binary images.  We
can also threshold the final image (p) at this point.  We set any
value 50 or above to 255, i.e., white, with

<<>>=
pp = pixThresholdToValue(p, 50, 255)
@

\begin{figure}[H]
<<>>=
plot(pp)
@
\caption{An example of removing ``smudges'' on an image by thresholding grey values to white using the function \code{pixThresholdToValue()} in \pkg{Rtesseract}.}
\label{fig:remove_smudge}
\end{figure}

When we plot the resulting image, the gray marks on the vertical lines have disappeared in Fig \ref{fig:remove_smudge}.
Note however that it removed some of the text, specifically the rotated text on the
right and the URL within that text that is colored blue in the
original image.
Fortunately, we don't care about this text for our
application.

We can compare the image we created with the lines removed to the one
Tesseract creates by passing the option \code{textord\_tabfind\_show\_vlines}.
We do this with

<<>>=
tesseract(smithburn, pageSegMode = "psm_auto",
          textord_tabfind_show_vlines = 1)
@

This creates a separate PDF file, by default named ``vhlinefinding.pdf'', which displays the vertical lines as detected by Tesseract.
%FIX: Need to discuss what this displays. It is 12 pages. Which pages are relevant.

%FIX: Does this still work in tesseract 4.0?
% It doesn't seem to be set when we query the resulting api object.
%   GetVariables(ts, "textord_tabfind_show_vlines")
% The following does work:
%
% ts = tesseract(smithburn, pageSegMode = "psm_auto")
% SetVariables(ts, textord_tabfind_show_vlines = 1)
% GetVariables(ts, "textord_tabfind_show_vlines")
% bb = GetBoxes(ts)
%  This writes:  Image resolution = 600, max line width = 30, min length=150
%
%

\begin{comment}
  
  \subsection{Augmenting the dictionaries}

There are a plethora of variables that can be adjusted, so for space
considerations we will highlight the ones we have found the most
useful in our work.

%% Might need to cut this - cannot get it to work correctly
To continue the example above, on inspection of
the results of the OCR there are some domain-specific terms that are
not being recognized correctly.  While it might be possible to correct
these in post-processing, it is preferable to tweak the settings of
the OCR engine to allow the correct recognition in the processing
stage.  The above document includes the two terms that are consistently
mis-recognized; {\it{``Anopheles''}} (a scientific name) and ``Zika''
(a virus name).  \code{Tesseract} allows
user-specified words to be added to the dictionary of correct matches
through a ``user-words'' file.  \pkg{Rtesseract} allows you to specify
this file, which will then be used by the OCR engine to resolve
ambiguities:

<<>>=
writeLines(c("2 z' 1 1 "), "eng.unicharambigs")
writeLines(c("Zika", "Anopheles"), "eng.user-words")
@

For comparison, we will create a second \code{TesseractBaseAPI} object
with new settings,

<<>>=
api2 = tesseract(smithburn, pageSegMode="psm_auto",
                 opts = list(user_words_file = file.path(getwd(),"eng.user-words"),
                             user_words_suffix = "user-words",
                             language_model_penalty_non_dict_word = 0.75,
                             # tessedit_char_blacklist = "a",
                             tessedit_enable_dict_correction = "1"))
@

And then we can check if the results are improved,

<<>>=
grep("^An", GetText(api), value = TRUE)
grep("^An", GetText(api2), value = TRUE)
@ 

Extending this idea, we can explore which characters have the lowest
confidence and/or occurrence in the text.

<<>>=
symbolBB = GetBoxes(api, level = "symbol")
symbolConf = sort(tapply(symbolBB[,"confidence"], row.names(symbolBB), median))
# Lowest eight 
symbolConf[1:8]
@

As before, visually inspecting these low-confidence characters using the \code{plotSubImage} function for a single bounding box, or the \code{plotSubsets} function to create a panel
plot of multiple bounding boxes can help verify that
they are correct or diagnose issues (e.g., Fig. \ref{fig:subsets}).

\begin{figure}[H]
<<fig = TRUE>>=
idx = which(symbolBB[,"confidence"] < 70)

plotSubsets(symbolBB[idx,], img = png::readPNG(f))
@
\caption{An example of the \code{plotSubsets} function, which creates panel plots of
  individual subsets of the image along with the associated OCR result
  and confidence.  In this example, the ``ã'' character is oft
  mis-recognized.}
\label{fig:subsets}
\end{figure}

Say we are primarily interested in the
abstract.  Using the bounding boxes or other coordinates as guides, we
are able to adjust the OCR-ed area without leaving \proglang{R}.  Typically, to do
this would involve partitioning the image into subsets using an image
editing tool (e.g., Gimp, Darktable, Photoshop), and then running the
OCR on each of these subset images.  This process involves leaving R
and involves steps that may be difficult to replicate.  However,
\pkg{Rtesseract} allows the user to specify a rectangle of interest
using the \code{SetRectangle} function.  This function takes a
\code{TesseractBaseAPI} object and the left, top, width and height of
the sub-area as additional arguments (the dimensions of the full input
image can be found with \code{GetImageDims}).  Once set, the next call to the
\code{TesseractBaseAPI} will only recognize text within this rectangle
- no external image manipulation required.  Since we know that the
abstract is between the ``Abstract'' header and the ``Keywords''
tagline, we can easily find the coordinates of these using the
bounding boxes and then only recognize text within this area:

<<>>=
bb = GetBoxes(api)
idx = grep("Abstract|Keywords", rownames(bb))
bb[idx,"top"]
GetImageDims(api)

SetRectangle(api, 0, 950, 4000, 1800 - 950)
GetText(api, level = "textline")

#Reset rectangle
GetImageDims(api)

SetRectangle(api, 0, 0, 4013, 3210)
@ 


\end{comment}

\section{Querying and setting variables}

Many additional customizable variables allow further fine control of the OCR engine
and are accessible through \proglang{R} via the \pkg{Rtesseract}
package.
These include white- and black-list
characters, custom user-added patterns and words, etc.
To find the current settings for the tesseract OCR engine we use the
function \code{PrintVariables()},

<<>>=
v = PrintVariables(api)
v
@ 

At the time of writing, this returns \Sexpr{length(PrintVariables())}
variables. \code{PrintVariables} returns a named character vector,
with the names corresponding to the variable name and the value
corresponding to the current value. We can search for options,

<<>>=
grep("blacklist", names(v), value = TRUE)
@

and then adjust those options via a call to
\code{SetVariables},

<<>>=
SetVariables(api, tessedit_char_blacklist = c("^", "_", "|"))
@

Alternatively, we can also set variables when the api is created by \code{tesseract()}

<<>>=
api = tesseract(smithburn, opts = list(tessedit_char_blacklist = c("^", "_", "|")))
@

Full documentation of user-adjustable variables
is available at
\url{https://github.com/tesseract-ocr/tesseract/blob/master/doc/tesseract.1.asc}.

\subsection{Specifying the recognized language}

Tesseract requires trained language files in order to operate.  
Depending on OS, \pkg{Rtesseract} by default searches \texttt{/usr/bin/local}, \texttt{/usr/local/share}, or \texttt{/usr/share/} directory for
installed trained language files, but when we create the
\code{TesseractBaseAPI} we can specify both a different
language, say Spanish (``spa'')

<<>>=
api = tesseract(smithburn, lang = "spa")
@

and an alternative location for the language data files,

<<eval = FALSE>>=
api = tesseract(smithburn, lang = "spa", datapath = ".")
@

We can find the current location where the API is searching for
language data with \code{GetDatapath()},

<<>>=
GetDatapath(api)
@

Since Tesseract allows us to create
trained data files for custom languages, the ability to specify the
path of the language files alleviates the need to place these language
data files in the default search location.  More information on
training Tesseract and providing language files can be found at
\url{https://github.com/tesseract-ocr/tesseract/wiki/Training-Tesseract}.  

\subsection{Evaluating different configurations}

Sometimes, optimal settings are found through experimentation.  By
creating persistent \code{TesseractBaseAPI} objects via the \code{tesseract()}
function, two or more \code{TesseractBaseAPI}s can be created for the
same image, e.g.

<<>>=
api = tesseract(smithburn,
                pageSegMode = "PSM_AUTO")
api2 = tesseract(smithburn,
                 pageSegMode = "PSM_SINGLE_BLOCK",
                 opts = list(
                     tessedit_char_whitelist = c(letters,
                                                 LETTERS,
                                                 0:9,
                                                 ".")))
@

Doing this allows us to compare the impact of various settings
to fine tune the OCR for individual cases.  Once a suitable set of
adjustments have been found, we can exported these settings using
\code{PrintVariables()}. These custom variables can then be easily set
using again \code{SetVariables()} or the \code{opts} argument in the
\code{tesseract()} function.

\subsection{Writing results directly to file}

\pkg{Rtesseract} includes functionality to export the results of OCR
to many file types, including BoxText, HOcr/HTML , OSD
(Orientation and Script Detection), TSV (tab separated values), and
PDF.  Of these, \code{toPDF()} is unique in that it creates a searchable
PDF document. For example,

<<eval = FALSE>>=
toPDF(api, file = "Smithburn")
@ 

will create a PDF file called ``Smithburn.pdf'' which has the results of the OCR as a searchable text layer.

\subsection{Metadata}

Lastly, \pkg{Rtesseract} allows users to access metadata about the OCR
engine and the input image.  The current version of Tesseract can be
returned with 

<<>>=
tesseractVersion()
@

while the capabilities for
reading various image formats via leptonica is provided by,

<<>>=
leptonicaImageFormats()
@ 

Information about the source image are also accessible through \proglang{R} using

<<>>=
GetInputName(api)
GetImageInfo(api)
GetImageDims(api)
GetSourceYResolution(api)
@

\section{Package Installation}

In order to use \pkg{Rtesseract}, users must first have both Tesseract
and its dependency, Leptonica, installed.  Tesseract is available from
\url{https://github.com/tesseract-ocr/} and Leptonica from
\url{https://github.com/DanBloomberg/leptonica}.  Pre-compiled
binaries are available for most platforms for the current stable
version (v3.05.01), though \pkg{Rtesseract} also requires libraries to
compile which are not often included those for Windows
platforms. Therefore, if installing pre-built binaries of v3.05.01 for
Windows is desired, we suggest installing Tesseract from the Rwinlib
repository (\url{https://github.com/rwinlib/tesseract}), which also
includes the required libraries for Windows.  As previously mentioned
\pkg{Rtesseract} also works with the development version of Tesseract
(4.00-beta), which is currently available from
\url{https://github.com/tesseract-ocr/tesseract/releases}. At the time
of writing, we are not aware of pre-built binaries for v4.00 which
include the required libraries as well. However, to build Tesseract
from source requires many additional pieces of software, including
leptonica >= v1.74, automake, libtool, pkg-config, aclocal, autoheader,
and autoconf-archive.  Additionally, users will also need to install
the trained data for the language(s) used in the documents to be
processed. Currently, there are pre-trained data sets for 140+
languages available from \url{https://github.com/tesseract-ocr}, though the trained language files are specific to the version of Tesseract being used (i.e., language files for v3.05 will not work with Tesseract v4.0).
Additionally, for Tesseract versions 4.0+, there are trained language files that prioritize computational speed (\url{https://github.com/tesseract-ocr/tessdata_fast}) or accuracy (\url{https://github.com/tesseract-ocr/tessdata_best}).
Users can also provide their own trained data.  For more information
about training Tesseract, please see
\url{https://github.com/tesseract-ocr/tesseract/wiki/Training-Tesseract}.
\pkg{Rtesseract} includes some functions that assist with training,
including \code{ReadBoxFile()}.  Finally, the \pkg{Rtesseract} package
itself can be installed from source at
\url{https://github.com/duncantl/Rtesseract}.


\section{Comparison to other packages}
<<child = "comparison.Rnw">>=


%@MATT: Here is a hastily written segment that we want to reword and integrate
% to outline the distinction between Rtesseract and other packages.
% It connects with the previous paragraphs, but I have written it independently for now.
The \pkg{Rtesseract} provides 
\begin{itemize}
\item high-level functions to extract data from images,
\item functions for evaluating the OCR results,
\item infrastructure for reconstructing the extracted elements into meaningful
higher-level data structures (e.g. columns of text, paragraphs, columns and rows of tables)  
% Need to make this so if we want to include it. May want to drop this.
\item functionality for pre-processing images to improve them for OCR and to 
   do related non-OCR extraction, e.g., finding horizontal and vertical lines.
\item programmatic interface that enables querying functionality and sound error handling
and recovery that enables the development of robust programmatic workflows.
\end{itemize}


More image processing packages for \R{} have become available
and one can use these pre-process images before OCR or to extract
additional elements such as lines.  However, the \pkg{Rtesseract}
functionality also makes this possible with a workflow
that combines leptonica and tesseract.
By providing access to the leptonica functionality,
the \pkg{Rtesseract} package allows us to manipulate and process images in the same format
that tesseract operate on and to work with images directly before and after
the OCR. 



@ 


% Different section title
\section{Future directions}

\pkg{Rtesseract} provides both easy access to basic OCR functionality
for simple use-cases as well as access to low-level control parameters
for more advanced uses.

By allowing recovery of additional
information generated by Tesseract, such as confidence levels and
bounding boxes, \pkg{Rtesseract} supports novel applications of
inputting data from images into \proglang{R} (e.g., spatial analysis of text).
Leveraging free, open-source software allows full integration of the
work-flow of text from images into \proglang{R}.

\bibliography{Rtesseract.bib}
\end{document}
